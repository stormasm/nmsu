# 

## Gradient Descent

Gradient Descent is a method for finding the maximum rate of change at a
particular point on a multidimensional surface. In the case of neural
networks, the surfaces of interest usually are high in dimension which
means that they have many dependent variables. The concept in calculus
of partial derivatives enables one to find the rate of change with
respect to a single variable. The gradient is defined to be the sum of
all the partial derivatives which points in the direction of maximum
change. For a detailed discussion of this subject see Appendix B.

## Method of Least Squares

Since back propagation is very similar to the method of least squares,
it will be explained in order to convey and justify the derivation of
the back propagation algorithm.

The method of least squares is a technique that is used to fit the best
function to a set of observational data. Since this data has been
observed, associated with the data is a small amount of noise. In the
case of this algorithm, the experimenter usually has a general idea of
what the function looks like. For example, the function may be a
combination of simple linear functions, however the coefficients of the
function are unknown.

$$f(x) = c_1 f_1(x)\ + \ c_2 f_2(x)\ + \ ..... \  + c_m f_m(x)$$

The method of determining how well a particular function fits a set of
data is calculated by adding up the squares of the errors at each of the
observation points. The goal is to minimize $E$ which is defined to be
the error.

$$E = \sum_{j = 1}^N (f(x_j) - y_j)^2$$

For a detailed discussion of this subject see Appendix C.

### The Notation

The number of input output patterns will vary from problem to problem.
In the case of adding two three bit numbers then the total number of
input patterns is two to the sixth power which is 64. To delineate input
output pattern number 3 from pattern number 59 each set of input output
pairs will be denoted by the letter **p**. The algorithm is based on
layered feedforward networks and to distinguish between the two layers
we use two letters **i** and **j**. Layer **i** is always the layer
immediately below layer **j**. Given any two layers, we could be looking
at the fourth unit in one particular layer $i = 4$ and the sixth unit in
the layer directly above it so $j = 6$.

For each particular pattern there is an input pattern and an output
pattern. The output pattern will be denoted by a **t** where the **t**
stands for the **teaching pattern** or **target pattern**. The other
letter that is used in the notation is the letter **o**. This letter
delineates the output pattern that is actually generated by the network
at any particular layer. The letter **i** is used only when dealing with
an actual input unit.

So, the following examples will delineate the above ideas.

$t_{pj}$, $p = 3$, $j = 4$ The fourth teacher unit in pattern 3.

$o_{pj}$, $p = 9$, $j = 2$ The second output unit in pattern 9.

$o_{pi}$, $p = 9$, $i = 1$ The first input unit in pattern 9.

Note once again, that the **i's** and **j's** strictly distinguish
relative ordering of particular layers in a network. So, when one sees
an **i** they know that it means the layer below a particular layer
where the **j** is referenced.

### The Sigmoid Function

During the forward pass the sum of all the weights coming into a
particular unit are calculated. However, this is not the value that is
stored as the input of that particular unit. Instead, the sigmoid of
that particular value is stored. The sigmoid function is defined as

$$1 \over {1 + e^ {-(\sum_i \ w_{ji} \ o_{pi})}}$$

The derivative of this particular function with respect to its total
input $net_{pj}$ is given by

$${\partial o_{pj} \over \partial net_{pj}} \ = \ o_{pj} \ (1 - o_{pj})$$

### The Algorithm

This section will describe the rule for changing the weights in the
network. The derivation of the rule is based on two main principles,
namely gradient descent and the method of least squares. The key point
is that the set of weights in the network defines the actual function
that is trying to be learned based upon a set of input output patterns.
By minimizing the error surface through each pass, one hopefully gets
closer to the actual function.

The formula for minimizing the error is exactly the same as the one
described in the section on least squares.

$$E_p \ = \ {1 \over 2} \sum_j \ {(t_{pj} - o_{pj}}) ^ 2$$

Note, that $t_{pj}$ is a particular unit in the teacher pattern which is
equivalent to the *observed value* $y_j$. $o_{pj}$ is a particular unit
in the actual output pattern, derived by the forward pass, which is
equivalent to *the function value* $f(x_j)$. The $1 \over 2$ is strictly
placed here for convenience.

We now need to determine how the error measure changes with respect to
each weight. We minimize E by taking a derivative of the error surface
with respect to each weight. Note that $t_{pj}$ is constant and does not
change.

$$\Delta w_{ji} = - \eta {\partial E_p \over \partial w_{ji}} =
- \eta \sum_j ({t_{pj} - o_{pj}}) {\partial o_{pj} \over \partial w_{ji}}$$

Thus the formula for changing weights is based on the idea of gradient
descent. This idea will be expanded in a future paper which deals
specifically with the ideas of least squares, directional derivatives
and back propagation. Also, please check PDP chapter eight for a
complete derivation.
